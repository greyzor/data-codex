{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avito demand prediction challenge\n",
    "\n",
    "Avito is one of Russia’s largest classified advertisements website.\n",
    "The idea of this challenge is to predict the demand for a product.\n",
    "\n",
    "<h3>Details:</h3>\n",
    "<p>In their fourth Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive.\n",
    "</p>\n",
    "<h3>Link:</h3>\n",
    "<p>Full description of the challenge is available here:</p>\n",
    "<a href=\"https://www.kaggle.com/c/avito-demand-prediction/overview\" target=\"_blank\">https://www.kaggle.com/c/avito-demand-prediction/overview</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "# Models Packages\n",
    "from sklearn import feature_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "# Tf-Idf\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "# Viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ## Occasionally (dev purpose only)\n",
    "    sys.path.insert(0, \"../..\")\n",
    "    import aisimplekit\n",
    "except ModuleNotFoundError as err:\n",
    "    print(\"\"\"[err] {err}\"\"\".format(err=err))\n",
    "    print(\"\"\"Try: `pip install aisimplekit`\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from aisimplekit.features.stats import *\n",
    "from aisimplekit.utils.memory import reduce_mem_usage\n",
    "from aisimplekit.cv.cv_kfold import cross_validate_lgbm_regressor\n",
    "import aisimplekit.features.tfidf as tfidf\n",
    "from aisimplekit.dnn.rnn import RnnTextModel, RnnModelType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Loading data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(gp, downsample_ratio=None, folder='../input/avito-demand-prediction'):\n",
    "    \"\"\" \"\"\"\n",
    "    print('Loading train/test')\n",
    "    train = pd.read_csv(folder+'/train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "    test = pd.read_csv(folder+'/test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n",
    "    train_index = train.index\n",
    "    test_index = test.index\n",
    "\n",
    "    if downsample_ratio is not None:\n",
    "        print('Downsampling: %s' % downsample_ratio)\n",
    "        assert downsample_ratio > 1.0\n",
    "        from sklearn.utils import resample\n",
    "        train = resample(train, replace=False, n_samples=int(len(train)/downsample_ratio), random_state=123)\n",
    "        test = resample(test, replace=False, n_samples=int(len(test)/downsample_ratio), random_state=123)\n",
    "        train_index = train.index\n",
    "        test_index = test.index\n",
    "\n",
    "    if gp is not None:\n",
    "        print('Merging train/gp and test/gp')\n",
    "        train = train.reset_index().merge(gp, on='user_id', how='left').set_index('item_id')\n",
    "        test = test.reset_index().merge(gp, on='user_id', how='left').set_index('item_id')\n",
    "\n",
    "    return (train, test, train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers: Feature Extraction: User aggregated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aggregated_metrics(folder='../input/avito-demand-prediction', save_features=False):\n",
    "    \"\"\" Computes 3 features:\n",
    "    - `avg_times_up_user` - how often the average item of the user has been put up for sale.\n",
    "    - `avg_days_up_user` - the average number of days an item from the user has been put up for sale.\n",
    "    - `n_user_items` - the number of items the user has put up for sale.\n",
    "    \"\"\"\n",
    "    print('1/6 Loading data')\n",
    "    used_cols = ['item_id', 'user_id', 'price']\n",
    "    train = pd.read_csv(folder+'/train.csv', usecols=used_cols)\n",
    "    train_active = pd.read_csv(folder+'/train_active.csv', usecols=used_cols)\n",
    "    test = pd.read_csv(folder+'/test.csv', usecols=used_cols)\n",
    "    test_active = pd.read_csv(folder+'/test_active.csv', usecols=used_cols)\n",
    "\n",
    "    print('2.1/6 Building concatenated dataframe: all_samples')\n",
    "    all_samples = pd.concat([train,train_active,test,test_active]).reset_index(drop=True)\n",
    "    all_samples.drop_duplicates(['item_id'], inplace=True)\n",
    "    del train_active; del test_active; gc.collect()\n",
    "\n",
    "    print('2.2/Aggregating price by user')\n",
    "    gp2 = None\n",
    "    gp3 = None\n",
    "    if False:\n",
    "        gp2 = all_samples.groupby('user_id')['price'].mean()\n",
    "        gp3 = all_samples.groupby('user_id')['price'].max()\n",
    "    all_samples.drop(['price'], inplace=True, axis=1)\n",
    "    \n",
    "    ## concatenate the train and test period data to one dataframe for easier processing\n",
    "    print('2.3/6 Loading/Building concatenated dataframe: all_periods')\n",
    "    train_periods = pd.read_csv(folder+'/periods_train.csv', parse_dates=['date_from', 'date_to'])\n",
    "    test_periods = pd.read_csv(folder+'/periods_test.csv', parse_dates=['date_from', 'date_to'])\n",
    "    all_periods = pd.concat([train_periods,test_periods])\n",
    "    del train_periods; del test_periods; gc.collect()\n",
    "\n",
    "    ## Compute features\n",
    "    print('3/6 Computing: days_up, days_up_sum, times_put_up')\n",
    "    all_periods['days_up'] = all_periods['date_to'].dt.dayofyear - all_periods['date_from'].dt.dayofyear\n",
    "\n",
    "    gp = all_periods.groupby(['item_id'])[['days_up']]\n",
    "    gp_df = pd.DataFrame()\n",
    "    gp_df['days_up_sum'] = gp.sum()['days_up']\n",
    "    gp_df['times_put_up'] = gp.count()['days_up']\n",
    "    gp_df.reset_index(inplace=True)\n",
    "    gp_df.rename(index=str, columns={'index': 'item_id'})\n",
    "\n",
    "    print('4/6 Merging')\n",
    "    all_periods.drop_duplicates(['item_id'], inplace=True)\n",
    "    all_periods = all_periods.merge(gp_df, on='item_id', how='left')\n",
    "    del gp; del gp_df; gc.collect()\n",
    "\n",
    "    ## We have an interesting but kind of useless feature now. As seen in the second venn diagram, there is no overlap at all between `train_active` (and with that `train_periods`) and `train` concerning *item* IDs.\n",
    "    ## For the feature to become useful, we somehow have to associate an item ID with a user ID.\n",
    "    all_periods = all_periods.merge(all_samples, on='item_id', how='left')\n",
    "\n",
    "    print('5/6 Computing metric 1 and 2')\n",
    "    ## Group items: Metrics 1/3 and 2/3\n",
    "    gp = all_periods.groupby(['user_id'])[['days_up_sum', 'times_put_up']].mean().reset_index() \\\n",
    "        .rename(index=str, columns={'days_up_sum': 'avg_days_up_user', 'times_put_up': 'avg_times_up_user'})\n",
    "    ## Metric 3/3\n",
    "    ## For our last feature, `n_user_items`, we just group by user ID and count the number of items.\n",
    "    ## We have to be careful to use `all_samples` instead of `all_periods` here because the latter does not contain the `train.csv` and `test.csv` samples.\n",
    "    print('6/6 Computing metric 3')\n",
    "    n_user_items = all_samples.groupby(['user_id'])[['item_id']].count().reset_index() \\\n",
    "        .rename(index=str, columns={'item_id': 'n_user_items'})\n",
    "    gp = gp.merge(n_user_items, on='user_id', how='outer')\n",
    "\n",
    "    ## Save the features\n",
    "    if save_features is True:\n",
    "        gp.to_csv('aggregated_features.csv', index=False)\n",
    "    ## Cleanup\n",
    "    del all_samples; del all_periods; del train; del test; gc.collect()\n",
    "    return (gp, gp2, gp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers: Feature Extraction:\n",
    "### User statistic features\n",
    "### Category statistic features\n",
    "### Statistics about geography counts, image counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_features(df):\n",
    "    ## Q2 - How many posts per user, divided by posts from his (region or city) ?\n",
    "    df = do_count(df, ['region', 'city'], 'X3', show_max=True);\n",
    "    df = do_count(df, ['user_id'], 'X4', show_max=True)\n",
    "    df['X5'] = df['X4']/df['X3']; df.drop(['X3','X4'],axis=1,inplace=True) # X5: \n",
    "    \"\"\" Q3 - How many posts having a description (or image or title or ...), divided by user's total posts ? \"\"\"\n",
    "    df = do_count(df, ['user_id'], 'X6', show_max=True)\n",
    "    df = do_countuniq(df, ['user_id'], 'image_top_1', 'X7', show_max=True)\n",
    "    df['X8'] = df['X7']/df['X6']; df.drop(['X7','X6'],axis=1,inplace=True) #\n",
    "\n",
    "    cols = ['description_num_chars', 'description_num_words', 'description_num_unique_words', 'description_words_vs_unique',\n",
    "            'title_num_chars', 'title_num_words', 'title_num_unique_words', 'title_words_vs_unique']\n",
    "    for col in cols:\n",
    "        df = do_mean(df, ['user_id'], col, 'mean_user_%s'%col, show_max=True)\n",
    "        df = do_mean(df, ['category_name'], col, 'mean_category_%s'%col, show_max=True)\n",
    "        df['ratio_mean_user-cat_%s'%col] = df['mean_user_%s'%col]/df['mean_category_%s'%col]\n",
    "        df.drop(['mean_user_%s'%col, 'mean_category_%s'%col],axis=1,inplace=True)\n",
    "    return df   ## Unique counts, Means; Variances, Min/max/Median ; Top-ranked (categorical); First/Last .. ; Previous/Next\n",
    "\n",
    "def add_category_features(df):\n",
    "    df['price_rank'] = df.groupby(['category_name'])['price'].rank(ascending=True) # 0.229789\n",
    "    return df\n",
    "\n",
    "def add_other_features(df):\n",
    "    if True:\n",
    "        df = do_count(df, ['region'], 'T0', show_max=True)\n",
    "        df = do_count(df, ['city'], 'T1', show_max=True)\n",
    "        df['T2'] = df['T1']/df['T0']; df.drop(['T0','T1'],axis=1,inplace=True)\n",
    "    if True:\n",
    "        df = do_count(df, ['image_top_1'], 'T5', show_max=True) # 0.229818\n",
    "        df = do_count(df, ['image_top_1', 'category_name'], 'T8', show_max=True) # 0.229776\n",
    "    return df\n",
    "\n",
    "def add_stat_features(df):\n",
    "    df = add_user_features(df)\n",
    "    df = add_category_features(df)\n",
    "    df = add_other_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = None\n",
    "DEV = True\n",
    "VALID = False # helps to find the best num_rounds !\n",
    "n_rounds = 2401 # identified during validation, WARNING: set early_stopping_rounds to 50 !\n",
    "#data_dir = '../input/avito-demand-prediction'\n",
    "data_dir = '~/.kaggle/competitions/avito-demand-prediction/'\n",
    "\n",
    "USE_IMAGE_FEATURES = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_memory = False\n",
    "max_features = 8000 # TF-IDF: Counts\n",
    "\n",
    "## Dev settings\n",
    "early_stopping_rounds = 15 # 50\n",
    "learning_rate = 0.05 # 0.019\n",
    "num_leaves = 128\n",
    "downsample_ratio = 8.0\n",
    "\n",
    "with_user_agg = False\n",
    "with_tfidf = True\n",
    "\n",
    "main_model = 'dnn' # 'lgb' or 'dnn'\n",
    "analyzer = 'word' # 'word' or 'char'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEV is False:\n",
    "    ## Same settings for VALID and SUBMIT\n",
    "    early_stopping_rounds = 50\n",
    "    learning_rate = 0.019\n",
    "    num_leaves = 250\n",
    "    downsample_ratio = None\n",
    "\n",
    "    with_user_agg = True\n",
    "\n",
    "    if main_model == 'dnn':\n",
    "        with_tfidf = False\n",
    "        \n",
    "    optimize_memory = True\n",
    "    max_features = None\n",
    "    sub_filename = \"lgb-base-2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading (or computing): user aggregated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using aggregated metrics.\n",
      "CPU times: user 544 µs, sys: 58 µs, total: 602 µs\n",
      "Wall time: 481 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gp = None\n",
    "gp2 = None\n",
    "gp3 = None\n",
    "agg_cols = []\n",
    "\n",
    "if gp is not None:\n",
    "    print('Reusing gp.')\n",
    "elif with_user_agg is True:\n",
    "    (gp, gp2, gp3) = compute_aggregated_metrics(folder=data_dir, save_features=False)\n",
    "    agg_cols = list(gp.columns)[1:]\n",
    "else:\n",
    "    print('Not using aggregated metrics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test\n",
      "Downsampling: 8.0\n",
      "Train shape: 187928 Rows, 16 Columns\n",
      "Test shape: 63554 Rows, 16 Columns\n",
      "Combining Train and Test\n",
      "Final dataframe shape: 251482 Rows, 16 Columns\n",
      "CPU times: user 47.3 s, sys: 3.54 s, total: 50.9 s\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(training, testing, traindex, testdex) = load_data(gp, downsample_ratio=downsample_ratio, folder=data_dir)\n",
    "\n",
    "if gp is not None and optimize_memory is True:\n",
    "    del(gp)\n",
    "    gc.collect()\n",
    "\n",
    "y = training.deal_probability.copy().clip(0.0, 1.0)\n",
    "training.drop(\"deal_probability\",axis=1, inplace=True)\n",
    "print('Train shape: {} Rows, {} Columns'.format(*training.shape))\n",
    "print('Test shape: {} Rows, {} Columns'.format(*testing.shape))\n",
    "print(\"Combining Train and Test\")\n",
    "\n",
    "df = pd.concat([training,testing],axis=0)\n",
    "del training, testing\n",
    "gc.collect()\n",
    "print('Final dataframe shape: {} Rows, {} Columns'.format(*df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-computed image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME !\n",
    "if USE_IMAGE_FEATURES is True:\n",
    "    df_img = pd.read_csv('../input/trainimgfeatv2/train_imgfeat_v2.csv')\n",
    "    df_img = df_img.rename(columns={'Unnamed: 0.1': 'image'})\n",
    "    df_img = df_img.drop('Unnamed: 0', axis=1)\n",
    "    df_img['image'] = df_img['image'].apply(lambda x: x.rstrip('.jpg'))\n",
    "    df_img['image'] = df_img['image'].apply(str)\n",
    "    df = df.reset_index().merge(df_img, on='image', how='left').set_index('item_id')\n",
    "    for col in df_img.columns:\n",
    "        df[col].fillna(-1.0, inplace=True)\n",
    "    del(df_img)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1/4] Feature Engineering: Simple transformer features (np.log, dates) + encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering\n",
      "Creating Time Variables..\n",
      "Encoding categorical variables\n",
      "CPU times: user 4.35 s, sys: 72.3 ms, total: 4.42 s\n",
      "Wall time: 4.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Feature Engineering\")\n",
    "df[\"price\"] = np.log(df[\"price\"]+0.001)\n",
    "df[\"price\"].fillna(-999,inplace=True)\n",
    "df[\"image_top_1\"].fillna(-999,inplace=True)\n",
    "\n",
    "print(\"Creating Time Variables..\")\n",
    "df[\"Weekday\"] = df['activation_date'].dt.weekday\n",
    "df[\"Weekd of Year\"] = df['activation_date'].dt.week\n",
    "df[\"Day of Month\"] = df['activation_date'].dt.day\n",
    "\n",
    "# Create Validation Index and Remove Dead Variables\n",
    "# training_index = df.loc[df.activation_date<=pd.to_datetime('2017-04-07')].index\n",
    "# validation_index = df.loc[df.activation_date>=pd.to_datetime('2017-04-08')].index\n",
    "df.drop([\"activation_date\", \"image\"],axis=1,inplace=True)\n",
    "\n",
    "print(\"Encoding categorical variables\")\n",
    "categorical = [\"user_id\", \"region\", \"city\", \"parent_category_name\", \"category_name\",\n",
    "               \"user_type\", \"image_top_1\", \"param_1\", \"param_2\", \"param_3\"]\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for col in categorical:\n",
    "    df[col] = lbl.fit_transform(df[col].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2/4] Feature Engineering: Simple Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.8 s, sys: 65.6 ms, total: 19.9 s\n",
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Meta Text Features\n",
    "textfeats = [\"description\", \"title\"]\n",
    "count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "\n",
    "for cols in textfeats:\n",
    "    df[cols] = df[cols].astype(str) \n",
    "    df[cols] = df[cols].astype(str).fillna('missing') # FILL NA\n",
    "    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n",
    "    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n",
    "    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n",
    "    df[cols + '_num_chars'] = df[cols].apply(len) # Count number of Characters\n",
    "    df[cols + '_num_desc_punct'] = df[cols].apply(lambda x: count(x, set(string.punctuation)))\n",
    "\n",
    "for col in agg_cols:\n",
    "    df[col].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3/4] Feature Engineering: Stats features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating by  ['region', 'city'] ...\n",
      "X3 max value =  10743\n",
      "Aggregating by  ['user_id'] ...\n",
      "X4 max value =  188\n",
      "Aggregating by  ['user_id'] ...\n",
      "X6 max value =  188\n",
      "Counting unique  image_top_1  by  ['user_id'] ...\n",
      "X7 max value =  91\n",
      "Calculating mean of  description_num_chars  by  ['user_id'] ...\n",
      "mean_user_description_num_chars max value =  3144.0\n",
      "Calculating mean of  description_num_chars  by  ['category_name'] ...\n",
      "mean_category_description_num_chars max value =  810.3571428571429\n",
      "Calculating mean of  description_num_words  by  ['user_id'] ...\n",
      "mean_user_description_num_words max value =  597.0\n",
      "Calculating mean of  description_num_words  by  ['category_name'] ...\n",
      "mean_category_description_num_words max value =  113.14880952380952\n",
      "Calculating mean of  description_num_unique_words  by  ['user_id'] ...\n",
      "mean_user_description_num_unique_words max value =  333.0\n",
      "Calculating mean of  description_num_unique_words  by  ['category_name'] ...\n",
      "mean_category_description_num_unique_words max value =  87.75892857142857\n",
      "Calculating mean of  description_words_vs_unique  by  ['user_id'] ...\n",
      "mean_user_description_words_vs_unique max value =  100.0\n",
      "Calculating mean of  description_words_vs_unique  by  ['category_name'] ...\n",
      "mean_category_description_words_vs_unique max value =  97.67952818096964\n",
      "Calculating mean of  title_num_chars  by  ['user_id'] ...\n",
      "mean_user_title_num_chars max value =  50.0\n",
      "Calculating mean of  title_num_chars  by  ['category_name'] ...\n",
      "mean_category_title_num_chars max value =  31.38918918918919\n",
      "Calculating mean of  title_num_words  by  ['user_id'] ...\n",
      "mean_user_title_num_words max value =  13.0\n",
      "Calculating mean of  title_num_words  by  ['category_name'] ...\n",
      "mean_category_title_num_words max value =  6.42159383033419\n",
      "Calculating mean of  title_num_unique_words  by  ['user_id'] ...\n",
      "mean_user_title_num_unique_words max value =  12.0\n",
      "Calculating mean of  title_num_unique_words  by  ['category_name'] ...\n",
      "mean_category_title_num_unique_words max value =  6.41961637334388\n",
      "Calculating mean of  title_words_vs_unique  by  ['user_id'] ...\n",
      "mean_user_title_words_vs_unique max value =  100.0\n",
      "Calculating mean of  title_words_vs_unique  by  ['category_name'] ...\n",
      "mean_category_title_words_vs_unique max value =  100.0\n",
      "Aggregating by  ['region'] ...\n",
      "T0 max value =  23281\n",
      "Aggregating by  ['city'] ...\n",
      "T1 max value =  10743\n",
      "Aggregating by  ['image_top_1'] ...\n",
      "T5 max value =  19321\n",
      "Aggregating by  ['image_top_1', 'category_name'] ...\n",
      "T8 max value =  3449\n",
      "CPU times: user 15.8 s, sys: 4.74 s, total: 20.5 s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Add statistical features\n",
    "df = add_stat_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4/4] Feature Engineering: TF-IDF Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_spec = {\n",
    "    'description': {\n",
    "        'vectorizer': tfidf.TRANSFORMER_TFIDF,\n",
    "        'ngram_range': (1,2),\n",
    "        'max_features': 17000,\n",
    "        'kwargs': {} # overridable\n",
    "    },\n",
    "    'title': {\n",
    "        'vectorizer': tfidf.TRANSFORMER_COUNT,\n",
    "        'ngram_range': (1,2),\n",
    "        'max_features': max_features,\n",
    "        'kwargs': None # no additional named args\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vocab_size: 25000\n",
      "CPU times: user 2min 27s, sys: 2.29 s, total: 2min 30s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(vectorizer, df_tfidf, tf_vocab) = tfidf.compute_features(df, transformer_spec, analyzer='word', stop=\"russian\")\n",
    "print('#vocab_size: %d' % len(tf_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model: LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 9 µs, total: 9 µs\n",
      "Wall time: 16.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if main_model == 'lgb':\n",
    "    # Drop Text Cols\n",
    "    try:\n",
    "        df.drop(textfeats, axis=1, inplace=True)\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Reduce Memory (See function up top)\n",
    "    df = reduce_mem_usage(df)\n",
    "\n",
    "    # Combine Dense Features with Sparse Text Bag of Words Features\n",
    "    print('Concatenating base features + tfidf features..')\n",
    "    if df_tfidf is not None:\n",
    "        X = hstack([csr_matrix(df.loc[traindex,:].values), df_tfidf[0:traindex.shape[0]]]) # Sparse Matrix\n",
    "        testing = hstack([csr_matrix(df.loc[testdex,:].values), df_tfidf[traindex.shape[0]:]])\n",
    "        predictors = df.columns.tolist() + tf_vocab\n",
    "    else:\n",
    "        X = hstack([csr_matrix(df.loc[traindex,:].values)]) # Sparse Matrix\n",
    "        testing = hstack([csr_matrix(df.loc[testdex,:].values)])\n",
    "        predictors = df.columns.tolist()\n",
    "\n",
    "    del(df)\n",
    "    del(df_tfidf)\n",
    "    gc.collect()\n",
    "\n",
    "    lgbm_params =  {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse', # 'max_depth': 15,\n",
    "        'num_leaves': num_leaves,\n",
    "        'feature_fraction': 0.50,\n",
    "        'bagging_fraction': 0.70, # 'bagging_freq': 5,\n",
    "        'learning_rate': learning_rate,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # Training and Validation Set\n",
    "    modelstart = time.time()\n",
    "    print('Training..')\n",
    "    if VALID == True:\n",
    "        print('Mode: Dev with Cross-validation')\n",
    "        ## CV: identify best tuning params, and features\n",
    "        (scores, best_num_rounds) = cross_validate_lgbm_regressor(\n",
    "                                        X.tocsr(), y, folds=3, repeats=1, predictors=predictors,\n",
    "                                        categorical=categorical, lgbm_params=lgbm_params,\n",
    "                                        num_boost_rounds=n_rounds,\n",
    "                                        early_stopping_rounds=early_stopping_rounds, verbose_eval=10\n",
    "                                    )\n",
    "        print(scores, best_num_rounds)\n",
    "        print('Average best round: {}'.format(np.mean(best_num_rounds)))\n",
    "\n",
    "    elif DEV == False:\n",
    "        print('Mode: Submit')\n",
    "        lgtrain = lgb.Dataset(X, y, feature_name=predictors, categorical_feature = categorical)\n",
    "        del(X)\n",
    "        gc.collect()\n",
    "\n",
    "        lgb_clf = lgb.train(lgbm_params, lgtrain, num_boost_round=n_rounds, verbose_eval=40)\n",
    "        lgpred = lgb_clf.predict(testing)\n",
    "\n",
    "        lgsub = pd.DataFrame(lgpred,columns=[\"deal_probability\"],index=testdex)\n",
    "        lgsub['deal_probability'].clip(0.0, 1.0, inplace=True) # Between 0 and 1\n",
    "        lgsub.to_csv(sub_filename,index=True,header=True)\n",
    "    \n",
    "    else:\n",
    "        print('Mode: Dev without Cross-validation')\n",
    "        ## DEV\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=23)\n",
    "        lgtrain = lgb.Dataset(X_train, y_train, feature_name=predictors, categorical_feature = categorical)\n",
    "        lgvalid = lgb.Dataset(X_valid, y_valid, feature_name=predictors, categorical_feature = categorical)\n",
    "        del(X)\n",
    "        del(X_train)\n",
    "        gc.collect()\n",
    "\n",
    "        lgb_clf = lgb.train(lgbm_params, lgtrain, num_boost_round=n_rounds, valid_sets=[lgtrain, lgvalid], valid_names=['train','valid'],\n",
    "            early_stopping_rounds=early_stopping_rounds, verbose_eval=10)\n",
    "\n",
    "        print('RMSE:', np.sqrt(metrics.mean_squared_error(y_valid, lgb_clf.predict(X_valid))))\n",
    "        del(X_valid)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model: RNN for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col: price\n",
      "False\n",
      "col: item_seq_number\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "num_cols = ['price', 'item_seq_number']\n",
    "for col in num_cols:\n",
    "    print('col: {}'.format(col))\n",
    "    print(df[col].isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO/ zeros in 'price' col ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "89a03cdb62c3    2.647760\n",
       "b19c7b1221b4    1.925407\n",
       "a9531307bfb5    2.139542\n",
       "ea75ef7aac9d    2.666167\n",
       "8e0368bccf5f    1.840283\n",
       "1a9e736d9d5f    2.197932\n",
       "46b9ab22e4d5    1.481884\n",
       "e33ad17e3afd    1.793532\n",
       "c31cbe9d818f    2.151867\n",
       "5b219b622efe    2.201566\n",
       "f3bfdc5d1659    1.555550\n",
       "cb2aa2f2db4c    2.819080\n",
       "5f1b7c04fa70    2.136220\n",
       "bc4249121261    2.185329\n",
       "3cc74acfd597    2.759381\n",
       "e1fb8fcb76c4    2.280294\n",
       "20a023fcf4e4    2.241968\n",
       "9abf9d365ce7         NaN\n",
       "a7552ff81fd8    2.526762\n",
       "b598d199df17    1.723691\n",
       "6ea51ad31c5c    2.067844\n",
       "96fbefe55968    2.713228\n",
       "26dee0228b3b    1.976108\n",
       "47ed054ee87c    2.323401\n",
       "af739158d7e4    2.021691\n",
       "8dc057499a08    2.021691\n",
       "e284ef01be04    1.875099\n",
       "00d3943b935c    2.197932\n",
       "f16256b3ed98    1.976108\n",
       "d704c9b0af25    1.793532\n",
       "                  ...   \n",
       "bf8e9dd8e089         NaN\n",
       "314d2145bf6b    1.591690\n",
       "716d9c1eeb25    1.925407\n",
       "efc230dc3cbe    2.001065\n",
       "bf15a29a0e7f    2.702087\n",
       "ba88539855df    1.840283\n",
       "78db2e967e7f    2.039220\n",
       "8eb6bc0a77fe    2.177481\n",
       "65e4e9dbae0b    2.011828\n",
       "eaf445bfb660    2.151867\n",
       "4936210fef51    2.208425\n",
       "0e052ae4f2c8    2.450715\n",
       "88cceae6bd57    2.294821\n",
       "86e0ac9491cc    2.205072\n",
       "7f8ff07b4801    2.323401\n",
       "8d45fdeb0401    2.090638\n",
       "ee1087e6642d    1.989232\n",
       "5a3697ad1eb4    2.809841\n",
       "a5393f049c46    1.793532\n",
       "7b63da4211db    1.976108\n",
       "5d706c188be1    2.167986\n",
       "9cfa6701b9bf    1.840283\n",
       "78fbdd9ac1f5    2.001065\n",
       "a0cc3afcb4b2    2.290698\n",
       "ef49ca65cab3    2.117847\n",
       "e30642a6a400    1.961397\n",
       "31a3c8d59b5d    2.194161\n",
       "94e00b71d273    2.021691\n",
       "1c2e1212eb7f    2.021691\n",
       "a99d5b61734b    2.610918\n",
       "Name: price, Length: 251482, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log1p(df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-999.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc['9abf9d365ce7'].price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No loss, nor metrics specified: using rmse by default!\n",
      "(236589, 300)\n",
      "(236589, 300)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 169135 samples, validate on 18793 samples\n",
      "Epoch 1/1\n",
      "169135/169135 [==============================] - 321s 2ms/step - loss: 0.2538 - root_mean_squared_error: 0.2538 - val_loss: 0.2340 - val_root_mean_squared_error: 0.2340\n",
      "Train on 169135 samples, validate on 18793 samples\n",
      "Epoch 1/1\n",
      "169135/169135 [==============================] - 299s 2ms/step - loss: 0.2336 - root_mean_squared_error: 0.2336 - val_loss: 0.2334 - val_root_mean_squared_error: 0.2334\n",
      "0    False\n",
      "dtype: bool\n",
      "0    False\n",
      "dtype: bool\n",
      "rmse for validation set: 0.2333799017978882\n",
      "63554/63554 [==============================] - 54s 849us/step\n",
      "RNN Prediction is done.\n",
      "(63554, 1)\n",
      "CPU times: user 27min 48s, sys: 4min 1s, total: 31min 50s\n",
      "Wall time: 20min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if main_model == 'dnn':\n",
    "    # inputs and preproc\n",
    "    tokenizer_num_words = 200000\n",
    "    cat_cols = ['region', 'city', 'category_name', 'parent_category_name',\n",
    "                'param_1', 'param123', 'image_top_1']\n",
    "    text_seq_cols = ['title_description']\n",
    "    num_cols = ['price', 'item_seq_number'] #, 'price', 'item_seq_number']\n",
    "    num_transform_spec = {\n",
    "        'price': lambda s: np.log1p(np.clip(s, 0.0, float('+inf'))) # clip by almost 0, then convert to log\n",
    "    } # dont transfor item_seq_number\n",
    "\n",
    "    max_seq_length = 100\n",
    "    text_spec = {}\n",
    "    # embeddings\n",
    "    embedding_file = '../data/cc.ru.300.vec'\n",
    "    embedding_dim1 = 300 # dimension for the fasttext embeddings we've downloaded using the makefile.\n",
    "    emb_out_size = 10 # embedding default output size for each embedding\n",
    "    # model\n",
    "    batch_size = 512*3\n",
    "    model_type = RnnModelType.GRU # GRU or LSTM\n",
    "    n_units = 50\n",
    "    # final layers tuning\n",
    "    dropout_0 = 0.1\n",
    "    dropout_1 = 0.1\n",
    "    ndense_0 = 512\n",
    "    ndense_1 = 64\n",
    "    final_layer_handler = None # dont override\n",
    "    # control over learning and optimization, loss, metrics\n",
    "    learning_rates = (0.009, 0.0045) # initial and final rates, evolution is exponential decay.\n",
    "    loss_fn = None # rmse by default\n",
    "    metrics_fns = None # [rmse] by default\n",
    "\n",
    "    # Custom dataframe preparation for Keras\n",
    "    def _prepare_df_handler(df):\n",
    "        \"\"\" \"\"\"\n",
    "        df['param_1'].fillna(value='missing', inplace=True)\n",
    "        df['param_1'] = df['param_1'].astype(str)\n",
    "        df['param_2'].fillna(value='missing', inplace=True)\n",
    "        df['param_2'] = df['param_2'].astype(str)\n",
    "        df['param_3'].fillna(value='missing', inplace=True)\n",
    "        df['param_3'] = df['param_3'].astype(str)\n",
    "        df['title_description']= (df['title']+\" \"+df['description']).astype(str)\n",
    "        df['param_1'].fillna(value='missing', inplace=True)\n",
    "        df['param_2'].fillna(value='missing', inplace=True)\n",
    "        df['param_3'].fillna(value='missing', inplace=True)\n",
    "        df['param123'] = (df['param_1']+'_'+df['param_2']+'_'+df['param_3']).astype(str)\n",
    "#        del(df['param_1']);  del(df['param_2']);  del(df['param_3']); gc.collect()\n",
    "        return df\n",
    "    \n",
    "    # Model\n",
    "    rnn = RnnTextModel(\n",
    "        tokenizer_num_words,\n",
    "        cat_cols=cat_cols,\n",
    "        num_cols=num_cols, num_transform_spec=num_transform_spec,\n",
    "        text_seq_cols=text_seq_cols, max_seq_length=max_seq_length,\n",
    "        embedding_file=embedding_file, embedding_dim1=embedding_dim1, emb_out_size=emb_out_size,\n",
    "        _prepare_df_handler=_prepare_df_handler,\n",
    "        batch_size=batch_size, model_type=model_type, n_units=n_units,\n",
    "        dropout_0=dropout_0, dropout_1=dropout_1, ndense_0=ndense_0, ndense_1=ndense_1,\n",
    "        learning_rates=learning_rates, loss_fn=loss_fn, metrics_fns=metrics_fns, # using rmse by default\n",
    "        final_layer_handler=final_layer_handler,\n",
    "        optimizer=\"adam\",\n",
    "        text_spec={}\n",
    "    )\n",
    "    \n",
    "    # Prepare dataframe for keras\n",
    "    df = rnn.init_predictor(df, traindex)\n",
    "    \n",
    "    # Fit RNN + evaluate model\n",
    "    v_rmse = rnn.fit(df.loc[traindex,:], y, n_iter=2, cv=False, test_size=0.10, random_state=23)\n",
    "\n",
    "    # Make Predictions\n",
    "    preds = rnn.predict(df.loc[testdex,:], verbose=1)\n",
    "\n",
    "    # Save results\n",
    "    sub = pd.DataFrame( columns = ['item_id', 'deal_probability'])\n",
    "    sub['item_id'] = df.loc[testdex,:].index\n",
    "    sub['deal_probability'] = preds\n",
    "    sub.to_csv(\"rnn-output.csv\", index=False)\n",
    "    ## TODO: Add K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id,deal_probability\r\n",
      "00187a01adde,0.26224333\r\n",
      "e49e7570f4c9,0.34400818\r\n",
      "1fbd09de1edb,0.039779127\r\n",
      "bcef66acf79f,0.12411514\r\n",
      "5553436386df,0.29632738\r\n",
      "65eaba16d707,0.05301839\r\n",
      "ca105133c9f2,0.032180697\r\n",
      "18a94481489a,0.44176075\r\n",
      "8730b73938fc,0.14992341\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10 rnn-output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
