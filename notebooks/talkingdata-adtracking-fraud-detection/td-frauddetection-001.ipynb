{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Summary of project</h2>\n",
    "<br/>\n",
    "<a href=\"https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection)\" target=\"_blank\">Link to Kaggle Competition</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os, sys\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aisimplekit helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ## Occasionally (dev purpose only)\n",
    "    sys.path.insert(0, \"../..\")\n",
    "    import aisimplekit\n",
    "except ModuleNotFoundError as err:\n",
    "    print(\"\"\"[err] {err}\"\"\".format(err=err))\n",
    "    print(\"\"\"Try: `pip install aisimplekit`\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aisimplekit.features.stats import *\n",
    "from aisimplekit.models.lgb import lgb_train_cv\n",
    "from aisimplekit.models.xgb import xgb_train_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom notebook-specific helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(frm, to, debug=True, test_ofst=0, dtypes={}):\n",
    "    \"\"\" \"\"\"\n",
    "    print('Loading train: #%d' % (to-frm))\n",
    "    df_train = pd.read_csv(TRAIN_PATH, parse_dates=['click_time'], skiprows=range(1,frm), nrows=to-frm, dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "    if debug != 0:\n",
    "        nrows = 100000\n",
    "        print('Loading test: #%d' % nrows)\n",
    "        test_df = pd.read_csv(TEST_PATH, skiprows=range(1,1+test_ofst), nrows=nrows, parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])        \n",
    "    elif debug == 0:\n",
    "        print('Loading test: all. Param test_ofst ignored.')\n",
    "        test_df = pd.read_csv(TEST_PATH, parse_dates=['click_time'], dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "    else: raise\n",
    "\n",
    "        # copy reference field\n",
    "    sub = pd.DataFrame()\n",
    "    sub['click_id'] = test_df['click_id'].astype('int')\n",
    "    len_train = len(df_train)\n",
    "    df_train = df_train.append(test_df)\n",
    "    return (df_train, test_df, len_train, sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats_features(df_train, predictors=['app', 'device', 'os', 'channel', 'hour', 'day']):\n",
    "    \"\"\" \"\"\"\n",
    "    print('Extracting stats features...')\n",
    "    df_train = do_countuniq( df_train, ['ip'], 'app', 'ip_uniq_app_count', 'uint8', show_max=True )\n",
    "    df_train = do_countuniq( df_train, ['ip'], 'channel', 'ip_uniq_chan_count', 'uint8', show_max=True )\n",
    "    df_train = do_count( df_train, ['ip', 'app'], 'ip_app_count', show_max=True )\n",
    "    gc.collect()\n",
    "\n",
    "    predictors.extend([col for col in df_train.columns if col.startswith('X')])\n",
    "    predictors.extend([col for col in df_train.columns if col.startswith('Z')])\n",
    "\n",
    "    if 'ip_tcount' in df_train.columns:\n",
    "        predictors.extend(['ip_tcount'])\n",
    "    if 'ip_app_count' in df_train.columns:\n",
    "        predictors.extend(['ip_app_count'])\n",
    "    if 'ip_app_os_count' in df_train.columns:\n",
    "        predictors.extend(['ip_app_os_count'])\n",
    "    if 'ip_tchan_count' in df_train.columns:\n",
    "        predictors.extend(['ip_tchan_count'])\n",
    "    if 'ip_app_os_var' in df_train.columns:\n",
    "        predictors.extend(['ip_app_os_var'])\n",
    "    if 'ip_app_channel_var_day' in df_train.columns:\n",
    "        predictors.extend(['ip_app_channel_var_day'])\n",
    "    if 'ip_app_channel_mean_hour' in df_train.columns:\n",
    "        predictors.extend(['ip_app_channel_mean_hour'])\n",
    "\n",
    "    return df_train, predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timeserie_features(df_train, predictors=['app', 'device', 'os', 'channel', 'hour', 'day']):\n",
    "    \"\"\" \"\"\"\n",
    "    ## Timeseries features\n",
    "    print('Extracting Timserie features...')\n",
    "\n",
    "    print('[1/2] Extracting nextClick')\n",
    "    D=2**26\n",
    "    df_train['category'] = (df_train['ip'].astype(str) + \"_\" + df_train['app'].astype(str) + \"_\" + df_train['device'].astype(str) \\\n",
    "        + \"_\" + df_train['os'].astype(str)).apply(hash) % D\n",
    "    click_buffer= np.full(D, 3000000000, dtype=np.uint32)\n",
    "\n",
    "    df_train['epochtime'] = df_train['click_time'].astype(np.int64) // 10 ** 9\n",
    "    next_clicks= []\n",
    "    for category, t in zip(reversed(df_train['category'].values), reversed(df_train['epochtime'].values)):\n",
    "        next_clicks.append(click_buffer[category]-t)\n",
    "        click_buffer[category]= t\n",
    "    del(click_buffer)\n",
    "    qq = list(reversed(next_clicks))\n",
    "\n",
    "    df_train.drop(['category'], axis=1, inplace=True)\n",
    "    df_train['nextClick'] = pd.Series(qq).astype('float32')\n",
    "    predictors.append('nextClick')\n",
    "\n",
    "    print('[2/2] Extracting nextClick_sameChan')\n",
    "    D=2**26\n",
    "    df_train['category'] = (df_train['ip'].astype(str) + \"_\" + df_train['channel'].astype(str) + \"_\" + df_train['device'].astype(str) \\\n",
    "        + \"_\" + df_train['os'].astype(str)).apply(hash) % D\n",
    "    click_buffer= np.full(D, 3000000000, dtype=np.uint32)\n",
    "\n",
    "    next_clicks= []\n",
    "    for category, t in zip(reversed(df_train['category'].values), reversed(df_train['epochtime'].values)):\n",
    "        next_clicks.append(click_buffer[category]-t)\n",
    "        click_buffer[category]= t\n",
    "    del(click_buffer)\n",
    "    qq = list(reversed(next_clicks))\n",
    "\n",
    "    df_train.drop(['category'], axis=1, inplace=True)\n",
    "    df_train['nextClick_sameChan'] = pd.Series(qq).astype('float32')\n",
    "    predictors.append('nextClick_sameChan')\n",
    "\n",
    "    return df_train, predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data stored in Kaggle\n",
    "#TRAIN_PATH = \".kaggle/competitions/talkingdata-adtracking-fraud-detection/train.csv\"\n",
    "#TEST_PATH = \".kaggle/competitions/talkingdata-adtracking-fraud-detection/test.csv\"\n",
    "\n",
    "## Data downloaded locally\n",
    "TRAIN_PATH = \"~/.kaggle/competitions/talkingdata-adtracking-fraud-detection/train.csv\"\n",
    "TEST_PATH = \"~/.kaggle/competitions/talkingdata-adtracking-fraud-detection/test.csv\"\n",
    "\n",
    "dtypes = {\n",
    "    'ip': 'uint32', 'app': 'uint16', 'device': 'uint16',\n",
    "    'os': 'uint16', 'channel': 'uint16', 'is_attributed': 'uint8',\n",
    "    'click_id': 'uint32',\n",
    "}\n",
    "\n",
    "debug = 2\n",
    "limit_features = False\n",
    "\n",
    "nrows=184903891-1\n",
    "frm = 144903891\n",
    "nchunk = 1000000 #2000000\n",
    "val_size=int(0.33*nchunk); # debug == 2\n",
    "df_val = None\n",
    "\n",
    "if debug == 0:\n",
    "    ## No cross-validation, all test data\n",
    "    nchunk = 40000000; val_size = 5000000\n",
    "    frm = 21500000 # day 2/4\n",
    "elif debug == 1:\n",
    "    ## With cross-validation\n",
    "    nchunk = 5000000; frm = 85000000 # day-1\n",
    "    val_size = 2000000; frm_val = 144903891\n",
    "\n",
    "## Train data boundaries (fraction corresponding to nchunk size).\n",
    "to = frm + nchunk\n",
    "test_ofst = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train: #1000000\n",
      "Loading test: #100000\n",
      "CPU times: user 57.9 s, sys: 6.95 s, total: 1min 4s\n",
      "Wall time: 1min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################################################################################\n",
    "#        LOADING DATA\n",
    "############################################################################################################\n",
    "(df_train, test_df, len_train, sub) = load_datasets(\n",
    "    frm, to, debug=debug,\n",
    "    test_ofst=test_ofst, dtypes=dtypes\n",
    ")\n",
    "\n",
    "if debug == 1:\n",
    "    print('************ Cross-validation: Loading data (#%d samp) ************'.format(val_size))\n",
    "    len_train = len(df_train) - len(test_df)\n",
    "    df_val = pd.read_csv(TRAIN_PATH, parse_dates=['click_time'], skiprows=range(1,frm_val),\n",
    "                         nrows=val_size, dtype=dtypes,\n",
    "                         usecols=['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed'])\n",
    "    df_train = df_train.append(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stats features...\n",
      "Counting unique  app  by  ['ip'] ...\n",
      "ip_uniq_app_count max value =  54\n",
      "Counting unique  channel  by  ['ip'] ...\n",
      "ip_uniq_chan_count max value =  93\n",
      "Aggregating by  ['ip', 'app'] ...\n",
      "ip_app_count max value =  1142\n",
      "CPU times: user 3.4 s, sys: 90.6 ms, total: 3.49 s\n",
      "Wall time: 3.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################################################################################\n",
    "#        FEATURE EXTRACTION (1/2: base stats)\n",
    "############################################################################################################\n",
    "df_train['hour'] = pd.to_datetime(df_train.click_time).dt.hour.astype('uint8')\n",
    "df_train['day'] = pd.to_datetime(df_train.click_time).dt.day.astype('uint8')\n",
    "df_train['minute'] = pd.to_datetime(df_train.click_time).dt.minute.astype('uint8')\n",
    "\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "predictors = ['app', 'device', 'os', 'channel', 'hour', 'day']\n",
    "\n",
    "df_train, predictors = extract_stats_features(df_train, predictors=predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature: next click period]\n",
      "Extracting Timserie features...\n",
      "[1/2] Extracting nextClick\n",
      "[2/2] Extracting nextClick_sameChan\n",
      "CPU times: user 15.9 s, sys: 827 ms, total: 16.7 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################################################################################\n",
    "#        FEATURE EXTRACTION (1/2: timserie stats)\n",
    "############################################################################################################\n",
    "print('[Feature: next click period]')\n",
    "(df_train, predictors) = extract_timeserie_features(df_train, predictors)\n",
    "\n",
    "df_train['minute'] = pd.to_datetime(df_train.click_time).dt.minute.astype('uint8')\n",
    "prev_len = len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "#        FEATURE SELECTION\n",
    "############################################################################################################\n",
    "if limit_features is True:\n",
    "    predictors = ['app','channel', 'X3', 'X0', 'nextClick',\n",
    "                  'os', 'nextClickPeriod', 'device', 'hour','day',\n",
    "                  'nextClick_sameChan', 'ip_app_count']\n",
    "    categorical = ['app','channel','os','device','hour','day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.8 ms, sys: 16.4 ms, total: 92.2 ms\n",
      "Wall time: 95.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "############################################################################################################\n",
    "#        TRAINING (1/3: Prepare data for LGB)\n",
    "############################################################################################################\n",
    "## Drop unnecessary columns\n",
    "df_train.drop(['click_time'], axis=1, inplace=True)\n",
    "df_train.drop(['epochtime'], axis=1, inplace=True)\n",
    "\n",
    "## Convert types\n",
    "df_train['ip_uniq_app_count'] = df_train['ip_uniq_app_count'].astype('uint16')\n",
    "df_train['ip_app_count'] = df_train['ip_app_count'].astype('uint16')\n",
    "df_train['ip_uniq_chan_count'] = df_train['ip_uniq_chan_count'].astype('uint16')\n",
    "\n",
    "## Learning Parmeters: LGB\n",
    "params = {\n",
    "    'learning_rate': 0.05,\n",
    "    #'is_unbalance': 'true', # replaced with scale_pos_weight argument\n",
    "    'num_leaves': 15,  # 2^max_depth - 1\n",
    "    'max_depth': 4,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 100,  # Number of bucbketed bin for feature values\n",
    "    'subsample': 0.7,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'scale_pos_weight': 50, # because training data is extremely unbalanced \n",
    "}\n",
    "\n",
    "## Target columns\n",
    "target = 'is_attributed'\n",
    "\n",
    "## Categorical columns\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour', 'day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670000, 15) (330000, 15) (100000, 15)\n",
      "Predictors:  ['app', 'device', 'os', 'channel', 'hour', 'day', 'ip_app_count', 'nextClick', 'nextClick_sameChan']\n",
      "Categorical: ['app', 'device', 'os', 'channel', 'hour', 'day']\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "#        TRAINING (2/3: Train/Test/CV Split)\n",
    "############################################################################################################\n",
    "test_df = df_train[len_train:]\n",
    "\n",
    "if debug == 1:\n",
    "    df_val = df_train[-val_size:]\n",
    "    df_train = df_train[:len_train]\n",
    "else:\n",
    "    df_val = df_train[(len_train-val_size):len_train]\n",
    "    df_train = df_train[:(len_train-val_size)]\n",
    "\n",
    "print(df_train.shape, df_val.shape, test_df.shape)\n",
    "print('Predictors:  %s' % predictors)\n",
    "print('Categorical: %s' % categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "#        TRAINING (3/3: Train LGB Model)\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing validation datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:1205: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/usr/local/lib/python3.7/dist-packages/lightgbm/basic.py:762: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[10]\ttrain's auc: 0.745747\tvalid's auc: 0.809269\n",
      "[20]\ttrain's auc: 0.884998\tvalid's auc: 0.879312\n",
      "[30]\ttrain's auc: 0.893823\tvalid's auc: 0.884725\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's auc: 0.943103\tvalid's auc: 0.922824\n",
      "\n",
      "Model Report\n",
      "bst1.best_iteration:  1\n",
      "auc: 0.9228239503331953\n",
      "0.9228239503331954\n"
     ]
    }
   ],
   "source": [
    "(bst, best_iteration, eval_score) = lgb_train_cv(\n",
    "    params, df_train, df_val, predictors, target, \n",
    "    objective='binary', \n",
    "    metrics='auc',\n",
    "    early_stopping_rounds=30,\n",
    "    verbose_eval=True, \n",
    "    num_boost_round=1000,\n",
    "    categorical_features=categorical\n",
    ")\n",
    "\n",
    "pred_val_1 = bst.predict(df_val[predictors], num_iteration=best_iteration)\n",
    "\n",
    "## Roc score\n",
    "score = roc_auc_score(df_val.is_attributed, pred_val_1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.7/dist-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.920719\tvalid-auc:0.904687\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 25 rounds.\n",
      "[5]\ttrain-auc:0.944757\tvalid-auc:0.932568\n",
      "[10]\ttrain-auc:0.960443\tvalid-auc:0.951543\n",
      "[15]\ttrain-auc:0.964595\tvalid-auc:0.955915\n",
      "[20]\ttrain-auc:0.96746\tvalid-auc:0.956836\n",
      "[25]\ttrain-auc:0.971455\tvalid-auc:0.957737\n",
      "[30]\ttrain-auc:0.973931\tvalid-auc:0.959764\n",
      "[35]\ttrain-auc:0.976851\tvalid-auc:0.960948\n",
      "[40]\ttrain-auc:0.978953\tvalid-auc:0.961042\n",
      "[45]\ttrain-auc:0.98089\tvalid-auc:0.961023\n",
      "[50]\ttrain-auc:0.982447\tvalid-auc:0.960863\n",
      "[55]\ttrain-auc:0.983517\tvalid-auc:0.961188\n",
      "[60]\ttrain-auc:0.984673\tvalid-auc:0.961275\n",
      "[65]\ttrain-auc:0.985393\tvalid-auc:0.961668\n",
      "[70]\ttrain-auc:0.98676\tvalid-auc:0.961098\n",
      "[75]\ttrain-auc:0.9875\tvalid-auc:0.961213\n",
      "[80]\ttrain-auc:0.987978\tvalid-auc:0.960703\n",
      "[85]\ttrain-auc:0.988528\tvalid-auc:0.96037\n",
      "Stopping. Best iteration:\n",
      "[64]\ttrain-auc:0.985372\tvalid-auc:0.961703\n",
      "\n",
      "0.9617033369772972\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {} # use default ones\n",
    "\n",
    "model = xgb_train_cv(\n",
    "    xgb_params, df_train, df_val, predictors, target,\n",
    "    objective='binary:logistic',\n",
    "    early_stopping_rounds=25,\n",
    "    num_boost_round=200,\n",
    "    verbose_eval=5\n",
    ")\n",
    "\n",
    "## Measure XGB Performance on cross-validation data\n",
    "import xgboost as xgb\n",
    "dvalid = xgb.DMatrix(df_val[predictors], df_val.is_attributed)\n",
    "\n",
    "## predictions on val\n",
    "pred_val_2 = model.predict(dvalid, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "## Roc score\n",
    "score = roc_auc_score(df_val.is_attributed, pred_val_2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 1.16s\tremaining: 3m 49s\n",
      "1:\ttotal: 2.59s\tremaining: 4m 16s\n",
      "2:\ttotal: 3.92s\tremaining: 4m 17s\n",
      "3:\ttotal: 4.97s\tremaining: 4m 3s\n",
      "4:\ttotal: 6.31s\tremaining: 4m 6s\n",
      "5:\ttotal: 7.33s\tremaining: 3m 56s\n",
      "6:\ttotal: 8.31s\tremaining: 3m 49s\n",
      "7:\ttotal: 9.59s\tremaining: 3m 50s\n",
      "8:\ttotal: 10.9s\tremaining: 3m 52s\n",
      "9:\ttotal: 12.4s\tremaining: 3m 54s\n",
      "10:\ttotal: 14s\tremaining: 4m\n",
      "11:\ttotal: 14.9s\tremaining: 3m 53s\n",
      "12:\ttotal: 16s\tremaining: 3m 50s\n",
      "13:\ttotal: 17.5s\tremaining: 3m 52s\n",
      "14:\ttotal: 18.7s\tremaining: 3m 50s\n",
      "15:\ttotal: 20.1s\tremaining: 3m 50s\n",
      "16:\ttotal: 21.1s\tremaining: 3m 47s\n",
      "17:\ttotal: 22s\tremaining: 3m 42s\n",
      "18:\ttotal: 22.9s\tremaining: 3m 37s\n",
      "19:\ttotal: 23.6s\tremaining: 3m 32s\n",
      "20:\ttotal: 24.4s\tremaining: 3m 27s\n",
      "21:\ttotal: 25.1s\tremaining: 3m 23s\n",
      "22:\ttotal: 26s\tremaining: 3m 19s\n",
      "23:\ttotal: 26.8s\tremaining: 3m 16s\n",
      "24:\ttotal: 27.8s\tremaining: 3m 14s\n",
      "25:\ttotal: 28.8s\tremaining: 3m 12s\n",
      "26:\ttotal: 30s\tremaining: 3m 12s\n",
      "27:\ttotal: 31.2s\tremaining: 3m 11s\n",
      "28:\ttotal: 32.3s\tremaining: 3m 10s\n",
      "29:\ttotal: 33.1s\tremaining: 3m 7s\n",
      "30:\ttotal: 34.5s\tremaining: 3m 7s\n",
      "31:\ttotal: 35.4s\tremaining: 3m 5s\n",
      "32:\ttotal: 36.2s\tremaining: 3m 3s\n",
      "33:\ttotal: 37.5s\tremaining: 3m 3s\n",
      "34:\ttotal: 38.4s\tremaining: 3m\n",
      "35:\ttotal: 39.4s\tremaining: 2m 59s\n",
      "36:\ttotal: 40.8s\tremaining: 2m 59s\n",
      "37:\ttotal: 41.7s\tremaining: 2m 57s\n",
      "38:\ttotal: 42.5s\tremaining: 2m 55s\n",
      "39:\ttotal: 43.2s\tremaining: 2m 52s\n",
      "40:\ttotal: 44s\tremaining: 2m 50s\n",
      "41:\ttotal: 44.8s\tremaining: 2m 48s\n",
      "42:\ttotal: 45.6s\tremaining: 2m 46s\n",
      "43:\ttotal: 46.4s\tremaining: 2m 44s\n",
      "44:\ttotal: 47.1s\tremaining: 2m 42s\n",
      "45:\ttotal: 47.9s\tremaining: 2m 40s\n",
      "46:\ttotal: 48.9s\tremaining: 2m 39s\n",
      "47:\ttotal: 49.7s\tremaining: 2m 37s\n",
      "48:\ttotal: 50.5s\tremaining: 2m 35s\n",
      "49:\ttotal: 51.4s\tremaining: 2m 34s\n",
      "50:\ttotal: 52.3s\tremaining: 2m 32s\n",
      "51:\ttotal: 53s\tremaining: 2m 30s\n",
      "52:\ttotal: 53.7s\tremaining: 2m 29s\n",
      "53:\ttotal: 54.5s\tremaining: 2m 27s\n",
      "54:\ttotal: 55.4s\tremaining: 2m 26s\n",
      "55:\ttotal: 56.2s\tremaining: 2m 24s\n",
      "56:\ttotal: 56.9s\tremaining: 2m 22s\n",
      "57:\ttotal: 57.7s\tremaining: 2m 21s\n",
      "58:\ttotal: 58.6s\tremaining: 2m 19s\n",
      "59:\ttotal: 59.4s\tremaining: 2m 18s\n",
      "60:\ttotal: 1m\tremaining: 2m 17s\n",
      "61:\ttotal: 1m 1s\tremaining: 2m 15s\n",
      "62:\ttotal: 1m 1s\tremaining: 2m 14s\n",
      "63:\ttotal: 1m 2s\tremaining: 2m 12s\n",
      "64:\ttotal: 1m 3s\tremaining: 2m 11s\n",
      "65:\ttotal: 1m 3s\tremaining: 2m 9s\n",
      "66:\ttotal: 1m 4s\tremaining: 2m 8s\n",
      "67:\ttotal: 1m 5s\tremaining: 2m 7s\n",
      "68:\ttotal: 1m 6s\tremaining: 2m 6s\n",
      "69:\ttotal: 1m 7s\tremaining: 2m 5s\n",
      "70:\ttotal: 1m 8s\tremaining: 2m 4s\n",
      "71:\ttotal: 1m 9s\tremaining: 2m 3s\n",
      "72:\ttotal: 1m 10s\tremaining: 2m 1s\n",
      "73:\ttotal: 1m 11s\tremaining: 2m 1s\n",
      "74:\ttotal: 1m 11s\tremaining: 1m 59s\n",
      "75:\ttotal: 1m 12s\tremaining: 1m 58s\n",
      "76:\ttotal: 1m 13s\tremaining: 1m 56s\n",
      "77:\ttotal: 1m 13s\tremaining: 1m 55s\n",
      "78:\ttotal: 1m 14s\tremaining: 1m 54s\n",
      "79:\ttotal: 1m 15s\tremaining: 1m 53s\n",
      "80:\ttotal: 1m 16s\tremaining: 1m 51s\n",
      "81:\ttotal: 1m 16s\tremaining: 1m 50s\n",
      "82:\ttotal: 1m 17s\tremaining: 1m 49s\n",
      "83:\ttotal: 1m 18s\tremaining: 1m 48s\n",
      "84:\ttotal: 1m 19s\tremaining: 1m 47s\n",
      "85:\ttotal: 1m 20s\tremaining: 1m 46s\n",
      "86:\ttotal: 1m 20s\tremaining: 1m 45s\n",
      "87:\ttotal: 1m 21s\tremaining: 1m 43s\n",
      "88:\ttotal: 1m 22s\tremaining: 1m 42s\n",
      "89:\ttotal: 1m 23s\tremaining: 1m 41s\n",
      "90:\ttotal: 1m 23s\tremaining: 1m 40s\n",
      "91:\ttotal: 1m 24s\tremaining: 1m 39s\n",
      "92:\ttotal: 1m 25s\tremaining: 1m 38s\n",
      "93:\ttotal: 1m 26s\tremaining: 1m 36s\n",
      "94:\ttotal: 1m 26s\tremaining: 1m 35s\n",
      "95:\ttotal: 1m 27s\tremaining: 1m 34s\n",
      "96:\ttotal: 1m 28s\tremaining: 1m 33s\n",
      "97:\ttotal: 1m 29s\tremaining: 1m 32s\n",
      "98:\ttotal: 1m 29s\tremaining: 1m 31s\n",
      "99:\ttotal: 1m 30s\tremaining: 1m 30s\n",
      "100:\ttotal: 1m 31s\tremaining: 1m 29s\n",
      "101:\ttotal: 1m 32s\tremaining: 1m 28s\n",
      "102:\ttotal: 1m 32s\tremaining: 1m 27s\n",
      "103:\ttotal: 1m 33s\tremaining: 1m 26s\n",
      "104:\ttotal: 1m 34s\tremaining: 1m 25s\n",
      "105:\ttotal: 1m 35s\tremaining: 1m 24s\n",
      "106:\ttotal: 1m 35s\tremaining: 1m 23s\n",
      "107:\ttotal: 1m 36s\tremaining: 1m 22s\n",
      "108:\ttotal: 1m 37s\tremaining: 1m 21s\n",
      "109:\ttotal: 1m 38s\tremaining: 1m 20s\n",
      "110:\ttotal: 1m 39s\tremaining: 1m 19s\n",
      "111:\ttotal: 1m 39s\tremaining: 1m 18s\n",
      "112:\ttotal: 1m 40s\tremaining: 1m 17s\n",
      "113:\ttotal: 1m 41s\tremaining: 1m 16s\n",
      "114:\ttotal: 1m 41s\tremaining: 1m 15s\n",
      "115:\ttotal: 1m 42s\tremaining: 1m 14s\n",
      "116:\ttotal: 1m 43s\tremaining: 1m 13s\n",
      "117:\ttotal: 1m 44s\tremaining: 1m 12s\n",
      "118:\ttotal: 1m 45s\tremaining: 1m 11s\n",
      "119:\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "120:\ttotal: 1m 46s\tremaining: 1m 9s\n",
      "121:\ttotal: 1m 47s\tremaining: 1m 8s\n",
      "122:\ttotal: 1m 48s\tremaining: 1m 7s\n",
      "123:\ttotal: 1m 48s\tremaining: 1m 6s\n",
      "124:\ttotal: 1m 49s\tremaining: 1m 5s\n",
      "125:\ttotal: 1m 50s\tremaining: 1m 4s\n",
      "126:\ttotal: 1m 51s\tremaining: 1m 4s\n",
      "127:\ttotal: 1m 52s\tremaining: 1m 3s\n",
      "128:\ttotal: 1m 52s\tremaining: 1m 2s\n",
      "129:\ttotal: 1m 53s\tremaining: 1m 1s\n",
      "130:\ttotal: 1m 54s\tremaining: 1m\n",
      "131:\ttotal: 1m 55s\tremaining: 59.4s\n",
      "132:\ttotal: 1m 56s\tremaining: 58.5s\n",
      "133:\ttotal: 1m 56s\tremaining: 57.5s\n",
      "134:\ttotal: 1m 57s\tremaining: 56.5s\n",
      "135:\ttotal: 1m 58s\tremaining: 55.6s\n",
      "136:\ttotal: 1m 58s\tremaining: 54.7s\n",
      "137:\ttotal: 1m 59s\tremaining: 53.8s\n",
      "138:\ttotal: 2m\tremaining: 52.9s\n",
      "139:\ttotal: 2m 1s\tremaining: 51.9s\n",
      "140:\ttotal: 2m 1s\tremaining: 51s\n",
      "141:\ttotal: 2m 2s\tremaining: 50.1s\n",
      "142:\ttotal: 2m 3s\tremaining: 49.2s\n",
      "143:\ttotal: 2m 4s\tremaining: 48.3s\n",
      "144:\ttotal: 2m 4s\tremaining: 47.4s\n",
      "145:\ttotal: 2m 5s\tremaining: 46.5s\n",
      "146:\ttotal: 2m 6s\tremaining: 45.6s\n",
      "147:\ttotal: 2m 7s\tremaining: 44.7s\n",
      "148:\ttotal: 2m 7s\tremaining: 43.8s\n",
      "149:\ttotal: 2m 8s\tremaining: 42.8s\n",
      "150:\ttotal: 2m 9s\tremaining: 41.9s\n",
      "151:\ttotal: 2m 9s\tremaining: 41s\n",
      "152:\ttotal: 2m 10s\tremaining: 40.1s\n",
      "153:\ttotal: 2m 11s\tremaining: 39.2s\n",
      "154:\ttotal: 2m 12s\tremaining: 38.3s\n",
      "155:\ttotal: 2m 12s\tremaining: 37.4s\n",
      "156:\ttotal: 2m 13s\tremaining: 36.6s\n",
      "157:\ttotal: 2m 14s\tremaining: 35.7s\n",
      "158:\ttotal: 2m 15s\tremaining: 34.8s\n",
      "159:\ttotal: 2m 16s\tremaining: 34s\n",
      "160:\ttotal: 2m 16s\tremaining: 33.1s\n",
      "161:\ttotal: 2m 17s\tremaining: 32.3s\n",
      "162:\ttotal: 2m 18s\tremaining: 31.4s\n",
      "163:\ttotal: 2m 19s\tremaining: 30.6s\n",
      "164:\ttotal: 2m 20s\tremaining: 29.7s\n",
      "165:\ttotal: 2m 21s\tremaining: 28.9s\n",
      "166:\ttotal: 2m 21s\tremaining: 28s\n",
      "167:\ttotal: 2m 22s\tremaining: 27.2s\n",
      "168:\ttotal: 2m 23s\tremaining: 26.3s\n",
      "169:\ttotal: 2m 24s\tremaining: 25.4s\n",
      "170:\ttotal: 2m 24s\tremaining: 24.5s\n",
      "171:\ttotal: 2m 25s\tremaining: 23.7s\n",
      "172:\ttotal: 2m 26s\tremaining: 22.8s\n",
      "173:\ttotal: 2m 26s\tremaining: 22s\n",
      "174:\ttotal: 2m 27s\tremaining: 21.1s\n",
      "175:\ttotal: 2m 28s\tremaining: 20.2s\n",
      "176:\ttotal: 2m 29s\tremaining: 19.4s\n",
      "177:\ttotal: 2m 29s\tremaining: 18.5s\n",
      "178:\ttotal: 2m 30s\tremaining: 17.7s\n",
      "179:\ttotal: 2m 31s\tremaining: 16.8s\n",
      "180:\ttotal: 2m 32s\tremaining: 16s\n",
      "181:\ttotal: 2m 32s\tremaining: 15.1s\n",
      "182:\ttotal: 2m 33s\tremaining: 14.3s\n",
      "183:\ttotal: 2m 34s\tremaining: 13.4s\n",
      "184:\ttotal: 2m 35s\tremaining: 12.6s\n",
      "185:\ttotal: 2m 36s\tremaining: 11.7s\n",
      "186:\ttotal: 2m 37s\tremaining: 10.9s\n",
      "187:\ttotal: 2m 37s\tremaining: 10.1s\n",
      "188:\ttotal: 2m 38s\tremaining: 9.22s\n",
      "189:\ttotal: 2m 39s\tremaining: 8.38s\n",
      "190:\ttotal: 2m 40s\tremaining: 7.54s\n",
      "191:\ttotal: 2m 40s\tremaining: 6.7s\n",
      "192:\ttotal: 2m 41s\tremaining: 5.86s\n",
      "193:\ttotal: 2m 42s\tremaining: 5.02s\n",
      "194:\ttotal: 2m 43s\tremaining: 4.18s\n",
      "195:\ttotal: 2m 43s\tremaining: 3.34s\n",
      "196:\ttotal: 2m 44s\tremaining: 2.51s\n",
      "197:\ttotal: 2m 45s\tremaining: 1.67s\n",
      "198:\ttotal: 2m 46s\tremaining: 836ms\n",
      "199:\ttotal: 2m 47s\tremaining: 0us\n",
      "0.9438385196919701\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "import sklearn\n",
    "\n",
    "cat_features_index = [0,1,2,3]\n",
    "\n",
    "def auc(m, train, test):\n",
    "    return (sklearn.metrics.roc_auc_score(y_train, m.predict_proba(train)[:,1]),\n",
    "                sklearn.metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]))\n",
    "\n",
    "model_catb = cb.CatBoostClassifier(\n",
    "    eval_metric=\"AUC\", depth=7, iterations=200,\n",
    "    l2_leaf_reg=4,\n",
    "    learning_rate=0.25,\n",
    "    od_type='Iter', od_wait=20,\n",
    "    one_hot_max_size=50\n",
    ")\n",
    "\n",
    "model_catb.fit(df_train[predictors], df_train[target], cat_features=cat_features_index)\n",
    "\n",
    "pred_val_3 = model_catb.predict_proba(df_val[predictors])[:,1]\n",
    "score = roc_auc_score(df_val[target], pred_val_3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Models: final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9632014158114894\n"
     ]
    }
   ],
   "source": [
    "w1 = 1/5.\n",
    "w2 = 2/5.\n",
    "w3 = 2/5.\n",
    "\n",
    "pred_stacked_cv = pred_val_1 * w1 + pred_val_2 * w2 + pred_val_3 * w3\n",
    "\n",
    "score = roc_auc_score(df_val[target], pred_stacked_cv)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = bst.predict(test_df[predictors], num_iteration=best_iteration)\n",
    "\n",
    "dtest = xgb.DMatrix(test_df[predictors])\n",
    "pred_2 = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "pred_3 = model_catb.predict_proba(test_df[predictors])[:,1]\n",
    "\n",
    "pred_stacked_test = pred_1 * w1 + pred_2 * w2 + pred_3 * w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving output to: out-dev-small.csv\n"
     ]
    }
   ],
   "source": [
    "############################################################################################################\n",
    "#        PREDICTIONS on TEST data.\n",
    "############################################################################################################\n",
    "fnames = {\n",
    "    0: 'final-output.csv',\n",
    "    1: 'out-dev-big.csv',\n",
    "    2: 'out-dev-small.csv'\n",
    "}\n",
    "\n",
    "fname = fnames[debug]\n",
    "print('Saving output to: {fname}'.format(fname=fname))\n",
    "sub[target] = pred_stacked_test\n",
    "sub.to_csv(fname, index=False, float_format='%.9f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catboost_info  out-dev-small.csv  td-frauddetection-001.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
